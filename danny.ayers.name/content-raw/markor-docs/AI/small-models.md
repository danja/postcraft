Regarding efficient pretrained models, I found two relevant models:Bittensor Language Model (BTLM-3B-8k-base): This is a 3 billion parameter language model, which can be quantized to 4-bit, fitting devices with as little as 3GB of memory. It outperforms models trained on significantly more tokens and provides performance comparable to open 7B parameter models.bigscience/T0_3B: This model, with 3 billion parameters, can be tuned on consumer hardware with 11GB of RAM, making it suitable for limited hardware resources.