A misconception, also sometimes polled by @elonmusk (for humor, I believe), is a false dichotomy between Transformers and Diffusion. Transformer is a neural network architecture. Diffusion is a particular way of modeling the data distribution. These are two separate things. You can train Diffusion models by using Transformers (Transformer U Nets). The actual dichotomy is between Autoregressive (AR) models vs Diffusion models. And the first is great for more abstract tokens: language, VQ or discrete quantized tokens at a patch/chunk level, etc. The latter is great for raw signals and continuous latent space embeddings, especially much higher dimensional. Both are iterative, but in different axes: AR iterates across sequence length by generating token by token, and diffusion tries multiple versions of the output with improvement at every step, but generates the sequence all at once.