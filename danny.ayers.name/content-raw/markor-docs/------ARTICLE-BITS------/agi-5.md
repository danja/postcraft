5 levels of preconceptions

I'm trying to pin down a psychological phenomenon I believe I have observed. It is a contrast in certain ways of thinking between people who have ADHD traits and those considered normal. I'll describe this to you by way of an example, I'd like you to identify any general patterns that may be at work.
The example that led me here is OpenAI's 5 steps towards artificial general intelligence. They have steps of conversational AI, reasoning, autonomy, innovating, organizational.
They sell a conversational AI service, and this is associated with their marketing, so clearly to some degree their roadmap is retrofitted to a path their company can be seen to be following. The 'organizational' level concerns artificial intelligence that is capable of performing the work of an entire organization. Again, this may be targeting the commercial world, but I've read other discussion of these steps justifying this trajectory. 
But as a person diagnosed with ADHD the steps here look fundamentally misguided. Firstly the notion of agi described is strongly tied to human-like intelligence. To me that seems wrong because what humans display is a special case arrived at by a particular set of circumstances, physics, evolution etc. I would start by trying to abstract out the concept we call intelligence. The generality part I would take as being the ability to reuse the same core system, adapting as necessary, to solve new tasks.
There aren't many examples to draw from, but we do have the natural kingdom at large. By rating creatures on how intelligent we would say they were,  noting the charactistics, we may get to a more useful definition that may also be applied to artificial intelligence.
With conversation seen as key to intelligence, again we have a human perspective. 
With reasoning, we already have logical reasoners, eg. A prolog engine. Reasoning seems tangential to the ability to converse.  Autonomy is an interesting one. Once wound up a mechanical clock will operate autonomously. In these steps the intention is for agents to do complex tasks, but the actual idea of what agency is doesn't seem to be captured. Innovation is difficult to abstract from the human context, but exploring and testing unknown systems is in there. Also use of tools, in humans and animals may offer clues. The organizational level has a lot I like. But if you consider that ant colonies are very successful organizations, but individual ants aren't considered verybintelligent. There's a mismatch 
Overall the approach takes a monolithic view of intelligence, as if agi Is a single person. In the real world, intelligence emerges in the context of societies. This aspect is fundamental, yet appears only in the last step. 
But when I try to think about how best to approach agi, It seems obvious that a more open approach is needed. All the steps are based on bad assumptions, and the conclusions carried up vertically.
I'm not entirely sure what you'd call the fallacies involved, but it seems like a systematic application of them.
So what I'd like to pin down are the modes of thinking employed. Contrasting neurotypical and neurodiverse.
People talk about thinking outside the box, this seems useful, modified a little. If the box is taken to be constraints and areas of knowledge, where those barriers are placed is significant. In these steps it feels like an overarching box is used, to only consider intelligence in human terms. This artificially limits exploration of the problem space, negatively impacts creativity.

