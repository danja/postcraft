I want to brainstorm ideas around extensions of testing and  automation. Somehow integrate logging and performance measuring code together with an extension mechanism to allow plugins. How might one create a subsystem whereby arbitrary metrics might be transparently included in program flow, so benchmarking was integral. The system should also include an evaluation card, specified in turtle rdf, that expressed the measurement parameters.
First please consider idea this in general, abstract terms and give it a suitable name. Then identify the necessary overall subsystem shape and abstract components, interfaces and composition method. Check design patterns that may be appropriate.
Then read the project knowledge and figure out a good way of integrating this into the transmissions engine, as well as individual processors in an unintrusive fashion.
Then tell me the next steps for implementation.

Change points in code identified as genes

