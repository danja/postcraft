

Browsing 5 months later and decided to log in to give this comment some love. Part of my AI pitch to newbies is there are TWO pillars of the GAI revolution.  The first pillar is the LLMs themselves, but the second pillar is embeddings.

The part of your comment that really hooked me though was when you stated LLMs are an application of this technology.  Facts!  I'm of the mindset that effective use of embeddings is what's going to initially simulate GAI conditions.

&#x200B;

Use case: "here's a bunch of text, and here's my question. what else do you need to know to solve this problem?"  Convert the response into an embedding, search your knowledge base for stuff to serve back, and BOOM, neural networks in action.  Future state, the initial model is an LLM trained to speak in embeddings (or function calling, maybe? lol), and you can skip the conversion and go directly to the kb. 

What a world. DM me if you ever want to talk about this shit more.