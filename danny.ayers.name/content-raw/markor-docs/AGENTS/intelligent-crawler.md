Intelligent Web Crawler

I would like to build an intelligent web crawler that takes a collection of a few hundred web links and builds a knowledge base and search engine with a scope limited to a group of topics.
The system will follow a procedure something like this: each link is dereferenced and the content saved. These will be categorised by topic. The topic areas considered of interest will be determined from this core corpus. Each of these pages will have all their links extracted and placed in a queue, and those linked pages in turn downloaded. These should be judged in terms of relevance to the topics in the core, with those of high relevance retained and their links added to the queue. The low-level aspects of the implementation will be determined later. An agent-oriented approach will be taken. What is needed now is to decide suitable algorithms for relevance and classification. A later task will be to create a dedicated llm style chat bot, so if something like the creation of embeddings or model fine-tuning maybe be useful at this initial stage it would be helpful to know good approaches. Please suggest what might be suitable algorithms, bearing in mind that they will be implemented as reusable agents.