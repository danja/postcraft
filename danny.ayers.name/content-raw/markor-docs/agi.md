No, we are not even close. AGI would require systems that
never hallucinate - when in doubt humans fill in with made-up
Reliably reason over abstractions - LLMs seem to be able, not reliably, but why is that necessary? Humans aren't.
can form long term plans - useful, but how does *size* determine General Intelligence
understand causality - transformers are pretty good at finding what follows from what happened before
reliably maintain models of the world - they can have a model, again, why does it have to be reliable
reliably handle outliers - good point, that I think I'd take as a marker.