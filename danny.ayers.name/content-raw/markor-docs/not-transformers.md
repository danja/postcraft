I've been seeing some people recently buzzing around a similar honeypot. My take is:
transformers are unexpectedly good. But general approach seems like leave them alone, attach bits outside to get cake today. But,
->
@EmekaOkoye @ylecun @RichardSSutton @DrJimFan @karpathy 

Perceptrons got hacked a bit to make CNNs, brilliant for images. Similar hack for RNNs, great but forgetful. LSTMs, a patch. Transformers, similar imho, a fairly arbitrary hack that happens to work very well.
->

LLMs are amazing. But they are trained on human language. They can find knowledge in it in a remarkable way. But language is a communication system between systems that contain knowledge. It's a representation of knowledge that works pretty well between entities with knowledge inside. Embedded in a biological system that took a long time to evolve

