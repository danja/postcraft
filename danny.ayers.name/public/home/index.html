<!DOCTYPE html>
<html lang="en">

<head>
    <title>Rawer</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <!-- #:todo remove when stable -->
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">

    <link rel="stylesheet" href="css/fonts.css" type="text/css" />
    <link rel="stylesheet" href="css/grid-columns.css" type="text/css" />
    <link rel="stylesheet" href="css/style.css" type="text/css" />
    <link rel="stylesheet" href="css/menu.css" type="text/css" />

</head>

<body>
    <header id="main-header">
        <h1 class="h-title">
           Raw<em>er</em>
        </h1>
    </header>
    <div class="grid-container">
        <div class="main-grid-item directory">
            <p><strong>Under Construction</strong></p>
            <p><em>there are many to-dos</em></p>
        </div>
        <div class="main-grid-item articles">
            <article>
                <!-- POST CONTENT TEMPLATE -->

<article class="post-content">
        <a href="">#</a>
    <h1>Semem Lives!</h1>
<p><em><a href="https://github.com/danja/semem">Semem</a> <del>is</del> will be an LLM-compatible context-aware, open-ended graph knowledgebase combining the advantages of vector embeddings and Linked Data technologies.</em></p>
<p><a href="https://danny.ayers.name/entries/2024-12-29_semem-ollama.html">Notes from yesterday</a>, <a href="https://danny.ayers.name/entries/2024-12-30_perplexed.html">this morning</a></p>
<p><strong>Wow!</strong>, that went remarkably well. I&#39;m pretty familiar with the semweb bits, the embedding/chat interfacing side very inexperienced. So, hard part (for me) first.</p>
<p>Claude did most of the heavy lifting, a long session about a month ago, a bunch this morning. I only actually <strong>ran it for the first time about an hour ago</strong>. Predictably Claude had left me with a load of manual <em>make-sane</em> refactoring to do. One stumble remaining is down to Faiss libs. The npm package <code>faiss</code> is an empty repo, Claude had hallucinated its API. What I needed was <code>faiss-node</code>. So that&#39;s where it stalls right now. I&#39;ve got the tooling in place to quicky sort this out. But I&#39;d used up my Claude Pro (!) token allowance by 11am, it&#39;s not due to reset for 20 mins and I have at least one tree to cut down before it gets dark...</p>
<p>Still embryonic, but so pleasing, in <a href="https://github.com/danja/semem/blob/main/src/OllamaExample.js">OllamaExample.js</a> there is :</p>
<pre><code class="language-javascript">chat: {
    provider: &#39;ollama&#39;,
    model: &#39;qwen2:1.5b&#39;
},
embedding: {
    provider: &#39;ollama&#39;,
    model: &#39;nomic-embed-text&#39;
}
...
    const prompt = &quot;What&#39;s the current state of AI technology?&quot;;
</code></pre>
<p><em>...and so...</em></p>
<pre><code class="language-sh">node src/OllamaExample.js

[INFO] Generating embedding...
[INFO] No existing interaction history found in JSON. Starting fresh.
[INFO] Memory initialized with 0 short-term and 0 long-term memories
[INFO] Extracting concepts...
[INFO] Extracted concepts: [ &#39;AI technology&#39; ]
[INFO] No interactions available
Response: AI (artificial intelligence) technology has come a long way since its inception in the 1950s. It is now a rapidly growing field, and there are numerous advancements being made across various industries.

1. Machine Learning: This involves programming computers to learn from data without being explicitly programmed. It&#39;s used for tasks like recognizing images, identifying voice commands, or predicting customer behavior.

2. Natural Language Processing (NLP): AI systems can understand human language in ways that make them easier to interact with and more efficient. They are used for tasks like chatbots, virtual assistants, and text-based search engines.

3. Robotics: As technology develops further, robots have become better at performing complex tasks autonomously. Some advanced robots now perform jobs such as manufacturing goods or even medical procedures.

4. Computer Vision (CV): This uses AI to help computers understand visual information in images or videos. It&#39;s used for applications like self-driving cars, security cameras, and facial recognition.

5. Deep Learning: This branch of artificial intelligence involves creating algorithms that can learn from data without being explicitly programmed. It&#39;s widely used in image classification tasks, natural language processing, and reinforcement learning.

6. Neural Networks: These are a type of machine learning algorithm inspired by the human brain. They&#39;re used for tasks such as recognizing handwriting, speech recognition, and playing games like chess or Go.

In summary, AI technology is advancing rapidly across various fields, offering more efficient solutions to complex problems. However, there&#39;s still room for improvement in areas like ethical considerations and ensuring that AI systems can be trusted by humans.
[INFO] Generating embedding...
[INFO] Extracting concepts...
[INFO] Extracted concepts: [
  &#39;Machine Learning&#39;,
  &#39;Natural Language Processing (NLP)&#39;,
  &#39;Robotics&#39;,
  &#39;Computer Vision (CV)&#39;,
  &#39;Deep Learning&#39;,
  &#39;Neural Networks&#39;
]
[INFO] Adding interaction: &#39;What&#39;s the current state of AI technology?&#39;
TypeError: Invalid the first argument type, must be an Array.
</code></pre>

</article>
<p class="post-title h-cinzel">
    <a href="">
        
    </a>
</p> <em></em><!-- POST CONTENT TEMPLATE -->

<article class="post-content">
        <a href="">#</a>
    <h1>Perplexed</h1>
<p><em>Before getting out of bed, I signed up for a free month of <a href="https://www.perplexity.ai/">Perplexity</a> and perplexed Claude</em></p>
<h2>Paths</h2>
<p> A note-to-self before I forget.</p>
<p> #:todo check how the <a href="https://eyereasoner.github.io/eye/">EYE</a> reasoner uses <a href="https://en.wikipedia.org/wiki/Leonhard_Euler">Euler</a> paths for #:semem</p>
<p> I had a few overlapping requirements for a classical reasoner. Long story short, I found <a href="https://eyereasoner.github.io/eye/">EYE</a> ticked many boxes. Notably it&#39;s <a href="https://en.wikipedia.org/wiki/Semantic_Web">semantic Web</a>-friendly.</p>
<p>I have a lot of unknowns (in the <a href="https://en.wikipedia.org/wiki/There_are_unknown_unknowns">Donald Rumsfeld senses</a>) around #:semem. I might have a short summary of what I&#39;m after :</p>
<p><strong>Semem is an LLM-compatible context-aware, open-ended graph knowledgebase combining the advantages of vector embeddings and Linked Data technologies.</strong></p>
<p>Right now I&#39;m still on the nuts &amp; bolts, I&#39;ve got fairly straightforward next steps to implement before the fun starts. The fun has many dimensions. From the above,</p>
<ul>
<li><strong>LLM-compatible</strong> - this starts with boilerplate connectors to Ollama, the OpenAI API, the <a href="https://www.anthropic.com/news/model-context-protocol">Model Context Protocol</a> etc. But an aspect of the <em>open-ended</em> I have in mind more broadly is exploring connectivity from a different perspective, that of inter-agent comms. This will go under the umbrella of #:LinguaFranche.  </li>
<li><strong>open-ended graph</strong> - this is the Web. Low-hanging fruit is hooking into existing SPARQL endpoints, eg. WikiData</li>
<li><strong>context-aware</strong> - I&#39;m working on the vocabulary around this to express it more clearly, but the general idea is that you want more relevant knowledge getting more attention</li>
</ul>
<p>Vector embeddings offer similarity measures, but the RDF (Web) model offers a whole different dimension.</p>
<p>Grrr! I went to LinkedIn to find a ref. related to this, instant distractions :</p>
<ul>
<li><a href="https://arxiv.org/abs/2411.03252">Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities</a> [sent to printer for later]</li>
<li><a href="https://chimezie.medium.com/declarative-prompts-llm-agents-and-their-programs-oh-my-aab1d0da6fef">Declarative Prompts, LLM Agents, and their Programs. Oh my…</a> [from Chimezie, left open in tab for sooner]</li>
</ul>
<p>A ref I can remember offhand is Bergi&#39;s material :</p>
<ul>
<li><a href="https://www.bergnet.org/2024/05/unified-landscape/">Unveiling the Unified Landscape: Bridging Knowledge Graphs and Machine Learning</a>  </li>
<li><a href="https://www.bergnet.org/2024/09/llm-kg-wombat/">Language Models Meet Knowledge Graphs - A Powerful Duo (plus a Wombat)</a></li>
<li><em>linked from there</em> <a href="https://towardsdatascience.com/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759">How to Implement Graph RAG Using Knowledge Graphs and Vector Databases</a> - Steve Hedden</li>
</ul>
<p>#:todo put some FOAF where <a href="https://www.bergnet.org/people/bergi/card">Bergi&#39;s card</a> links to it : <a href="http://danny.ayers.name/index.rdf#me">http://danny.ayers.name/index.rdf#me</a>
#:todo W6 link on <a href="https://hyperdata.it/xmlns/">https://hyperdata.it/xmlns/</a> is broken - make broken link finder #:transmission?</p>
<p>Enough gab, I&#39;d better get on with this thing.</p>

</article>
<p class="post-title h-cinzel">
    <a href="">
        
    </a>
</p> <em></em><!-- POST CONTENT TEMPLATE -->

<article class="post-content">
        <a href="">#</a>
    <h1>Sunny Day</h1>
<p><em>It&#39;s a sunny Sunday day.</em></p>
<p>My must-dos for today are plumbing. Actual plumbing :</p>
<ul>
<li>Sealant on leaky pipe at No.7</li>
<li>Fix the <em>second</em> hole I hadn&#39;t seen under the sink here at #:New place</li>
</ul>
<p>I also really need to make a start on some tree removal while the weather&#39;s dry. Stretching the notion of plumbing, first I need to sort out climbing gear, ropes, pulleys etc.</p>
<p>Financial plumbing, I need to sort a stack of papers, find the relevant forms I have somewhere, to sort out pension flow from Derbyshire County Council. I had a small (but very useful) monthly sum coming from there. Caroline worked at a DCC special needs place for a few years, I inherited her pension. It stalled a couple of months ago, I missed some kind of update step. I did do a little towards sorting it out, in the process discovered that I&#39;m probably due some funds from the years I worked at High Peak College.</p>
<p>Software plumbing...things to join together. Separate posts here for them.</p>

</article>
<p class="post-title h-cinzel">
    <a href="">
        
    </a>
</p> <em></em><!-- POST CONTENT TEMPLATE -->

<article class="post-content">
        <a href="">#</a>
    <h1>Semem Ollama</h1>
<p>#:semem, the <strong>Semantic Memory</strong> component of #:hyperdata I&#39;m working on is a high priority for me, there are a lot of places I want to use it.</p>
<p>I want to try and get the most out of <a href="https://en.wikipedia.org/wiki/Large_language_model">Large Language Model</a> and <a href="https://en.wikipedia.org/wiki/Linked_data">Linked Data</a> integration. It&#39;s a seriously fertile space, huge potential, lots of <strong>blue sky</strong> for research. I&#39;ve got a stack of ideas to try (many roughed out in scattered notes), but I&#39;m following the dev strategy of prioritizing activities for which I have an immediate use. This has led me to <a href="https://en.wikipedia.org/wiki/SPARQL">SPARQL</a> store-backed Graph <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> as the inroad. About a year ago, to help get my head around this, I made a little <a href="https://www.llamaindex.ai/">LlamaIndex</a> extension, <a href="https://github.com/danja/nlp/tree/main/GraphRAG">GraphRAG with SPARQL</a>. This was only a minimal proof-of-concept, but did give me a mental foothold.</p>
<p>The majority of the current activity in this space seems focused on using LLMs to derive <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">RDF</a> models from text (this is what the bits above did). For my needs I&#39;m putting the emphasis more on aligning LLM-interpreted text with existing ontologies. Ok, I&#39;m developing ontologies in a just-in-time fashion, but still treating the resources there as &quot;digital&quot; (logically defined) anchor points for the &quot;analog&quot; (probabilistic) shapes around LLMs.</p>
<p>I made a couple of underlying design choices around my #:hyperdata uber-project. One is to favour Javascript. Not everyone&#39;s preferred language (see <a href="https://www.youtube.com/watch?v=03lRzf7iSiU">What&#39;s Your Least Favourite Programming Language? (2024 soundcheck question) - Computerphile</a>), especially with AI dev being Python-first. But as I need to use JS in the browser it&#39;s a way of reducing my cognitive load (I&#39;m really bad at remembering syntax). Another is to do things kind-of from scratch, avoiding frameworks (libs ok). I&#39;m <a href="https://en.wikipedia.org/wiki/Eating_your_own_dog_food">dogfooding</a> to the max because I want to understand what&#39;s going on.</p>
<p>A key part of #:semem is embeddings derived from text. A key tool I need asap is a similarity-based search across my scattered docs (so I&#39;m not continually repeating myself). <a href="https://github.com/trustgraph-ai/trustgraph">TrustGraph</a> is broadly the kind of system I&#39;m aiming for, but that&#39;s big and quite blackboxy. While I was still going around in circles at a lower level, trying to decide which <strong>vector DB</strong> tooling to use, I stumbled on <a href="https://github.com/caspianmoon/memoripy">memoripy</a>. As the name suggests, it&#39;s Python. But it&#39;s an understandable size, has the general shape I&#39;m after and uses specific well-known libs (eg. <a href="https://github.com/facebookresearch/faiss">Faiss</a>) that all have <strong>JS APIs</strong>.</p>
<p>So for an initial setup for #:semem, the other week I got Claude to help me port it to JS. Once I&#39;d got what appeared to be necessary coverage, I left it there, untested.</p>
<p>Coming back to it today I see most of the examples use the OpenAI API. I don&#39;t have any credit on that right now. But <a href="https://github.com/ollama/ollama">Ollama</a> is also an option. I&#39;ve got that set up locally, should do nicely to kickstart #:semem.</p>
<p>I had a quick skim of the available small <a href="https://ollama.com/search">models</a>. One I hadn&#39;t noticed before is <a href="https://ollama.com/library/nomic-embed-text">nomic-embed-text</a>. Ok, that looks a good candidate for my embeddings. When playing, I got very promising results with <code>orca</code>, <code>phi</code> and <code>qwen</code>. They still seem to be up there in the populars, so I some have initial LLM candidates already installed.
I can&#39;t remember when I last update Ollama itself, I&#39;d better do that now.
The <a href="https://github.com/ollama/ollama/blob/main/docs/linux.md">manual install docs</a> have this :</p>
<blockquote>
If you are upgrading from a prior version, you should remove the old libraries with sudo rm -rf /usr/lib/ollama first.

<p>Download and extract the package:   </p>
<p>curl -L <a href="https://ollama.com/download/ollama-linux-amd64.tgz">https://ollama.com/download/ollama-linux-amd64.tgz</a> -o ollama-linux-amd64.tgz
sudo tar -C /usr -xzf ollama-linux-amd64.tgz</p>
</blockquote>

<p>That&#39;ll take a while on this connection.</p>
<p>Dogwalk time.</p>
<hr>
<pre><code class="language-sh">ollama serve
Error: listen tcp 127.0.0.1:11434: bind: address already in use
</code></pre>
<p>Reboot o&#39;clock.</p>
<pre><code class="language-sh">danny@danny-desktop:~$ ollama run qwen2:1.5b  
&gt;&gt;&gt; how are you?
I am an AI language model. How can I assist you today?
</code></pre>
<p>Sooo...</p>
<pre><code class="language-sh">ollama pull nomic-embed-text
...
curl http://localhost:11434/api/embeddings -d &#39;{
  &quot;model&quot;: &quot;nomic-embed-text&quot;,
  &quot;prompt&quot;: &quot;The sky is blue because of Rayleigh scattering&quot;
}&#39;
</code></pre>
<p>...which produces a lot of numbers. <strong>Yay!</strong></p>
<p>The <a href="https://ollama.com/library/nomic-embed-text">Ollama page for nomic-embed-text</a> has the cURL line above as well as py &amp; JS snippets. It also links to a <a href="https://www.nomic.ai/blog/posts/nomic-embed-text-v1">nomic blog post</a> about it. <a href="https://docs.nomic.ai/reference/api/embed-text-v-1-embedding-text-post">Nomic have an API</a> for using the model, but I&#39;ll stick with Ollama&#39;s since I want to use it for GPT models as well.</p>
<p>I believe <a href="https://github.com/facebookresearch/faiss">Faiss</a> has all the similarity search bits I need to get going on that front.</p>
<h2>Visualization</h2>
<p>Nomic have a Data Mapping (#:visualization) blog post series, this one&#39;s very handy for me : <a href="https://www.nomic.ai/blog/posts/see-your-data-with-dimensionality-reduction">See Your Data With Dimensionality Reduction</a>. I&#39;ve read papers on various algorithms but could only remember trad <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> off the top of my head. Mentioned here are also <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>, Hinton &amp; co&#39;s algorithm from 2008, <a href="https://arxiv.org/abs/1802.03426">UMAP</a> from 2018 and one which appears to be proprietary, Nomic Project. I&#39;m not sure if I&#39;ve encountered UMAP before, but it sounds like a good choice for my purposes.</p>
<p>The next post in the series is <a href="https://www.nomic.ai/blog/posts/why-are-web-browsers-the-best-data-browsers">Why Are Web Browsers The Best Data Browsers?</a>, which, after preliminaries goes on to describe their own (open sourced) <a href="https://github.com/nomic-ai/deepscatter">deepscatter</a>. It looks good, but I&#39;ve been around the block a few times with this stuff.</p>
<p>I&#39;m using <a href="https://rdf-ext.org">RDF-Ext</a> (RDF JS libs) nearby, and in there Bergi has a [RdfNetwork component]<a href="https://examples.rdf-ext.org/rdf-elements/">RDF-Ext</a>. This is built on <a href="https://js.cytoscape.org/">Cytoscape.js</a>, which is a mature renderer from bioinformatics circles, so should be plenty good on scalability and scatterplotty things.</p>
<p>#:todo what was that cytoscape plugin that looked good for pipelines from somewhere around Eclipse?
#:todo check out the cytoscape/<a href="https://mermaid.js.org/ecosystem/integrations-community.html">mermaid integrations</a></p>
<p>Ok, I guess I should actually look at what I&#39;ve got for #:semem code so far...</p>

</article>
<p class="post-title h-cinzel">
    <a href="">
        
    </a>
</p> <em></em><!-- POST CONTENT TEMPLATE -->

<article class="post-content">
        <a href="">#</a>
    <h1>Prill</h1>
<p>#:TIL the word <a href="https://en.wiktionary.org/wiki/prill">prill</a>, &quot;a pellet, a granule, a small bead&quot;, while watching this: <a href="https://youtu.be/dhW4XFGQB4o">Iron knife made from bacteria</a>. </p>
<p>The ety of #:prill :</p>
<blockquote>
<p>Unknown. OED mentions Cornish pryl (“sheep-droppings”) as a likely loan from English.</p>
</blockquote>
<p>CF. #:nurdle</p>
<p>The guy&#39;s <a href="https://primitivetechnology.wordpress.com/">Primitive Technology blog</a> and <a href="https://youtube.com/@primitivetechnology9550">videos</a> <strong>turn on subtitles</strong> are great for us <em>poltroniste</em>  (armchair enthusiasts (including chair-oriented politicians)).</p>
<p>Having to look it up, this came to mind:</p>
<blockquote>
<p>Text is cheap, knowledge is worth it.</p>
</blockquote>

</article>
<p class="post-title h-cinzel">
    <a href="">
        
    </a>
</p> <em></em>
            </article>
        </div>
        <div class="main-grid-item about">
            <!--
            <h2>About</h2>
            
            -->
        </div>
    </div>
    <script src="js/menu.js"></script>
</body>

</html>