<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Danny Ayers' Blog</title>
    
    <link href="https://danny.ayers.name"/>
    <updated>2024-12-30T12:04:15.464Z</updated>
    <id>https://danny.ayers.name</id>
    <author>
        <name>Danny Ayers</name>
        <email>danny.ayers@gmail.com</email>
    </author>

    
    <entry>
        <title>Entry 2024-12-30_perplexed</title>
        <link href="https://danny.ayers.name/entries/2024-12-30_perplexed.html"/>
        <id>https://danny.ayers.name/entries/2024-12-30_perplexed.html</id>
        <updated>2024-12-30T12:04:15.464Z</updated>
        <content type="html">&lt;!-- POST CONTENT TEMPLATE --&gt;

&lt;article class=&quot;post-content&quot;&gt;
        &lt;a href=&quot;&quot;&gt;#&lt;/a&gt;
    &lt;h1&gt;Perplexed&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Before getting out of bed, I signed up for a free month of &lt;a href=&quot;https://www.perplexity.ai/&quot;&gt;Perplexity&lt;/a&gt; and perplexed Claude&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Paths&lt;/h2&gt;
&lt;p&gt; A note-to-self before I forget.&lt;/p&gt;
&lt;p&gt; #:todo check how the &lt;a href=&quot;https://eyereasoner.github.io/eye/&quot;&gt;EYE&lt;/a&gt; reasoner uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Leonhard_Euler&quot;&gt;Euler&lt;/a&gt; paths for #:semem&lt;/p&gt;
&lt;p&gt; I had a few overlapping requirements for a classical reasoner. Long story short, I found &lt;a href=&quot;https://eyereasoner.github.io/eye/&quot;&gt;EYE&lt;/a&gt; ticked many boxes. Notably it&amp;#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Semantic_Web&quot;&gt;semantic Web&lt;/a&gt;-friendly.&lt;/p&gt;
&lt;p&gt;I have a lot of unknowns (in the &lt;a href=&quot;https://en.wikipedia.org/wiki/There_are_unknown_unknowns&quot;&gt;Donald Rumsfeld senses&lt;/a&gt;) around #:semem. I might have a short summary of what I&amp;#39;m after :&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Semem is an LLM-compatible context-aware, open-ended graph knowledgebase combining the advantages of vector embeddings and Linked Data technologies.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Right now I&amp;#39;m still on the nuts &amp;amp; bolts, I&amp;#39;ve got fairly straightforward next steps to implement before the fun starts. The fun has many dimensions. From the above,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LLM-compatible&lt;/strong&gt; - this starts with boilerplate connectors to Ollama, the OpenAI API, the &lt;a href=&quot;https://www.anthropic.com/news/model-context-protocol&quot;&gt;Model Context Protocol&lt;/a&gt; etc. But an aspect of the &lt;em&gt;open-ended&lt;/em&gt; I have in mind more broadly is exploring connectivity from a different perspective, that of inter-agent comms. This will go under the umbrella of #:LinguaFranche.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;open-ended graph&lt;/strong&gt; - this is the Web. Low-hanging fruit is hooking into existing SPARQL endpoints, eg. WikiData&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;context-aware&lt;/strong&gt; - I&amp;#39;m working on the vocabulary around this to express it more clearly, but the general idea is that you want more relevant knowledge getting more attention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Vector embeddings offer similarity measures, but the RDF (Web) model offers a whole different dimension.&lt;/p&gt;
&lt;p&gt;Grrr! I went to LinkedIn to find a ref. related to this, instant distractions :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2411.03252&quot;&gt;Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities&lt;/a&gt; [sent to printer for later]&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://chimezie.medium.com/declarative-prompts-llm-agents-and-their-programs-oh-my-aab1d0da6fef&quot;&gt;Declarative Prompts, LLM Agents, and their Programs. Oh my…&lt;/a&gt; [from Chimezie, left open in tab for sooner]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A ref I can remember offhand is Bergi&amp;#39;s material :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.bergnet.org/2024/05/unified-landscape/&quot;&gt;Unveiling the Unified Landscape: Bridging Knowledge Graphs and Machine Learning&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.bergnet.org/2024/09/llm-kg-wombat/&quot;&gt;Language Models Meet Knowledge Graphs - A Powerful Duo (plus a Wombat)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;linked from there&lt;/em&gt; &lt;a href=&quot;https://towardsdatascience.com/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759&quot;&gt;How to Implement Graph RAG Using Knowledge Graphs and Vector Databases&lt;/a&gt; - Steve Hedden&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;#:todo put some FOAF where &lt;a href=&quot;https://www.bergnet.org/people/bergi/card&quot;&gt;Bergi&amp;#39;s card&lt;/a&gt; links to it : &lt;a href=&quot;http://danny.ayers.name/index.rdf#me&quot;&gt;http://danny.ayers.name/index.rdf#me&lt;/a&gt;
#:todo W6 link on &lt;a href=&quot;https://hyperdata.it/xmlns/&quot;&gt;https://hyperdata.it/xmlns/&lt;/a&gt; is broken - make broken link finder #:transmission?&lt;/p&gt;
&lt;p&gt;Enough gab, I&amp;#39;d better get on with this thing.&lt;/p&gt;

&lt;/article&gt;
&lt;p class=&quot;post-title h-cinzel&quot;&gt;
    &lt;a href=&quot;&quot;&gt;
        
    &lt;/a&gt;
&lt;/p&gt; &lt;em&gt;&lt;/em&gt;</content>
    </entry>
    
    <entry>
        <title>Entry 2024-12-29_sunny-day</title>
        <link href="https://danny.ayers.name/entries/2024-12-29_sunny-day.html"/>
        <id>https://danny.ayers.name/entries/2024-12-29_sunny-day.html</id>
        <updated>2024-12-30T12:04:15.464Z</updated>
        <content type="html">&lt;!-- POST CONTENT TEMPLATE --&gt;

&lt;article class=&quot;post-content&quot;&gt;
        &lt;a href=&quot;&quot;&gt;#&lt;/a&gt;
    &lt;h1&gt;Sunny Day&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;It&amp;#39;s a sunny Sunday day.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;My must-dos for today are plumbing. Actual plumbing :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sealant on leaky pipe at No.7&lt;/li&gt;
&lt;li&gt;Fix the &lt;em&gt;second&lt;/em&gt; hole I hadn&amp;#39;t seen under the sink here at #:New place&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also really need to make a start on some tree removal while the weather&amp;#39;s dry. Stretching the notion of plumbing, first I need to sort out climbing gear, ropes, pulleys etc.&lt;/p&gt;
&lt;p&gt;Financial plumbing, I need to sort a stack of papers, find the relevant forms I have somewhere, to sort out pension flow from Derbyshire County Council. I had a small (but very useful) monthly sum coming from there. Caroline worked at a DCC special needs place for a few years, I inherited her pension. It stalled a couple of months ago, I missed some kind of update step. I did do a little towards sorting it out, in the process discovered that I&amp;#39;m probably due some funds from the years I worked at High Peak College.&lt;/p&gt;
&lt;p&gt;Software plumbing...things to join together. Separate posts here for them.&lt;/p&gt;

&lt;/article&gt;
&lt;p class=&quot;post-title h-cinzel&quot;&gt;
    &lt;a href=&quot;&quot;&gt;
        
    &lt;/a&gt;
&lt;/p&gt; &lt;em&gt;&lt;/em&gt;</content>
    </entry>
    
    <entry>
        <title>Entry 2024-12-29_semem-ollama</title>
        <link href="https://danny.ayers.name/entries/2024-12-29_semem-ollama.html"/>
        <id>https://danny.ayers.name/entries/2024-12-29_semem-ollama.html</id>
        <updated>2024-12-30T12:04:15.464Z</updated>
        <content type="html">&lt;!-- POST CONTENT TEMPLATE --&gt;

&lt;article class=&quot;post-content&quot;&gt;
        &lt;a href=&quot;&quot;&gt;#&lt;/a&gt;
    &lt;h1&gt;Semem Ollama&lt;/h1&gt;
&lt;p&gt;#:semem, the &lt;strong&gt;Semantic Memory&lt;/strong&gt; component of #:hyperdata I&amp;#39;m working on is a high priority for me, there are a lot of places I want to use it.&lt;/p&gt;
&lt;p&gt;I want to try and get the most out of &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot;&gt;Large Language Model&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Linked_data&quot;&gt;Linked Data&lt;/a&gt; integration. It&amp;#39;s a seriously fertile space, huge potential, lots of &lt;strong&gt;blue sky&lt;/strong&gt; for research. I&amp;#39;ve got a stack of ideas to try (many roughed out in scattered notes), but I&amp;#39;m following the dev strategy of prioritizing activities for which I have an immediate use. This has led me to &lt;a href=&quot;https://en.wikipedia.org/wiki/SPARQL&quot;&gt;SPARQL&lt;/a&gt; store-backed Graph &lt;a href=&quot;https://en.wikipedia.org/wiki/Retrieval-augmented_generation&quot;&gt;RAG&lt;/a&gt; as the inroad. About a year ago, to help get my head around this, I made a little &lt;a href=&quot;https://www.llamaindex.ai/&quot;&gt;LlamaIndex&lt;/a&gt; extension, &lt;a href=&quot;https://github.com/danja/nlp/tree/main/GraphRAG&quot;&gt;GraphRAG with SPARQL&lt;/a&gt;. This was only a minimal proof-of-concept, but did give me a mental foothold.&lt;/p&gt;
&lt;p&gt;The majority of the current activity in this space seems focused on using LLMs to derive &lt;a href=&quot;https://en.wikipedia.org/wiki/Resource_Description_Framework&quot;&gt;RDF&lt;/a&gt; models from text (this is what the bits above did). For my needs I&amp;#39;m putting the emphasis more on aligning LLM-interpreted text with existing ontologies. Ok, I&amp;#39;m developing ontologies in a just-in-time fashion, but still treating the resources there as &amp;quot;digital&amp;quot; (logically defined) anchor points for the &amp;quot;analog&amp;quot; (probabilistic) shapes around LLMs.&lt;/p&gt;
&lt;p&gt;I made a couple of underlying design choices around my #:hyperdata uber-project. One is to favour Javascript. Not everyone&amp;#39;s preferred language (see &lt;a href=&quot;https://www.youtube.com/watch?v=03lRzf7iSiU&quot;&gt;What&amp;#39;s Your Least Favourite Programming Language? (2024 soundcheck question) - Computerphile&lt;/a&gt;), especially with AI dev being Python-first. But as I need to use JS in the browser it&amp;#39;s a way of reducing my cognitive load (I&amp;#39;m really bad at remembering syntax). Another is to do things kind-of from scratch, avoiding frameworks (libs ok). I&amp;#39;m &lt;a href=&quot;https://en.wikipedia.org/wiki/Eating_your_own_dog_food&quot;&gt;dogfooding&lt;/a&gt; to the max because I want to understand what&amp;#39;s going on.&lt;/p&gt;
&lt;p&gt;A key part of #:semem is embeddings derived from text. A key tool I need asap is a similarity-based search across my scattered docs (so I&amp;#39;m not continually repeating myself). &lt;a href=&quot;https://github.com/trustgraph-ai/trustgraph&quot;&gt;TrustGraph&lt;/a&gt; is broadly the kind of system I&amp;#39;m aiming for, but that&amp;#39;s big and quite blackboxy. While I was still going around in circles at a lower level, trying to decide which &lt;strong&gt;vector DB&lt;/strong&gt; tooling to use, I stumbled on &lt;a href=&quot;https://github.com/caspianmoon/memoripy&quot;&gt;memoripy&lt;/a&gt;. As the name suggests, it&amp;#39;s Python. But it&amp;#39;s an understandable size, has the general shape I&amp;#39;m after and uses specific well-known libs (eg. &lt;a href=&quot;https://github.com/facebookresearch/faiss&quot;&gt;Faiss&lt;/a&gt;) that all have &lt;strong&gt;JS APIs&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So for an initial setup for #:semem, the other week I got Claude to help me port it to JS. Once I&amp;#39;d got what appeared to be necessary coverage, I left it there, untested.&lt;/p&gt;
&lt;p&gt;Coming back to it today I see most of the examples use the OpenAI API. I don&amp;#39;t have any credit on that right now. But &lt;a href=&quot;https://github.com/ollama/ollama&quot;&gt;Ollama&lt;/a&gt; is also an option. I&amp;#39;ve got that set up locally, should do nicely to kickstart #:semem.&lt;/p&gt;
&lt;p&gt;I had a quick skim of the available small &lt;a href=&quot;https://ollama.com/search&quot;&gt;models&lt;/a&gt;. One I hadn&amp;#39;t noticed before is &lt;a href=&quot;https://ollama.com/library/nomic-embed-text&quot;&gt;nomic-embed-text&lt;/a&gt;. Ok, that looks a good candidate for my embeddings. When playing, I got very promising results with &lt;code&gt;orca&lt;/code&gt;, &lt;code&gt;phi&lt;/code&gt; and &lt;code&gt;qwen&lt;/code&gt;. They still seem to be up there in the populars, so I some have initial LLM candidates already installed.
I can&amp;#39;t remember when I last update Ollama itself, I&amp;#39;d better do that now.
The &lt;a href=&quot;https://github.com/ollama/ollama/blob/main/docs/linux.md&quot;&gt;manual install docs&lt;/a&gt; have this :&lt;/p&gt;
&lt;blockquote&gt;
If you are upgrading from a prior version, you should remove the old libraries with sudo rm -rf /usr/lib/ollama first.

&lt;p&gt;Download and extract the package:   &lt;/p&gt;
&lt;p&gt;curl -L &lt;a href=&quot;https://ollama.com/download/ollama-linux-amd64.tgz&quot;&gt;https://ollama.com/download/ollama-linux-amd64.tgz&lt;/a&gt; -o ollama-linux-amd64.tgz
sudo tar -C /usr -xzf ollama-linux-amd64.tgz&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That&amp;#39;ll take a while on this connection.&lt;/p&gt;
&lt;p&gt;Dogwalk time.&lt;/p&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;ollama serve
Error: listen tcp 127.0.0.1:11434: bind: address already in use
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reboot o&amp;#39;clock.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;danny@danny-desktop:~$ ollama run qwen2:1.5b  
&amp;gt;&amp;gt;&amp;gt; how are you?
I am an AI language model. How can I assist you today?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sooo...&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;ollama pull nomic-embed-text
...
curl http://localhost:11434/api/embeddings -d &amp;#39;{
  &amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text&amp;quot;,
  &amp;quot;prompt&amp;quot;: &amp;quot;The sky is blue because of Rayleigh scattering&amp;quot;
}&amp;#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;...which produces a lot of numbers. &lt;strong&gt;Yay!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://ollama.com/library/nomic-embed-text&quot;&gt;Ollama page for nomic-embed-text&lt;/a&gt; has the cURL line above as well as py &amp;amp; JS snippets. It also links to a &lt;a href=&quot;https://www.nomic.ai/blog/posts/nomic-embed-text-v1&quot;&gt;nomic blog post&lt;/a&gt; about it. &lt;a href=&quot;https://docs.nomic.ai/reference/api/embed-text-v-1-embedding-text-post&quot;&gt;Nomic have an API&lt;/a&gt; for using the model, but I&amp;#39;ll stick with Ollama&amp;#39;s since I want to use it for GPT models as well.&lt;/p&gt;
&lt;p&gt;I believe &lt;a href=&quot;https://github.com/facebookresearch/faiss&quot;&gt;Faiss&lt;/a&gt; has all the similarity search bits I need to get going on that front.&lt;/p&gt;
&lt;h2&gt;Visualization&lt;/h2&gt;
&lt;p&gt;Nomic have a Data Mapping (#:visualization) blog post series, this one&amp;#39;s very handy for me : &lt;a href=&quot;https://www.nomic.ai/blog/posts/see-your-data-with-dimensionality-reduction&quot;&gt;See Your Data With Dimensionality Reduction&lt;/a&gt;. I&amp;#39;ve read papers on various algorithms but could only remember trad &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;PCA&lt;/a&gt; off the top of my head. Mentioned here are also &lt;a href=&quot;https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding&quot;&gt;t-SNE&lt;/a&gt;, Hinton &amp;amp; co&amp;#39;s algorithm from 2008, &lt;a href=&quot;https://arxiv.org/abs/1802.03426&quot;&gt;UMAP&lt;/a&gt; from 2018 and one which appears to be proprietary, Nomic Project. I&amp;#39;m not sure if I&amp;#39;ve encountered UMAP before, but it sounds like a good choice for my purposes.&lt;/p&gt;
&lt;p&gt;The next post in the series is &lt;a href=&quot;https://www.nomic.ai/blog/posts/why-are-web-browsers-the-best-data-browsers&quot;&gt;Why Are Web Browsers The Best Data Browsers?&lt;/a&gt;, which, after preliminaries goes on to describe their own (open sourced) &lt;a href=&quot;https://github.com/nomic-ai/deepscatter&quot;&gt;deepscatter&lt;/a&gt;. It looks good, but I&amp;#39;ve been around the block a few times with this stuff.&lt;/p&gt;
&lt;p&gt;I&amp;#39;m using &lt;a href=&quot;https://rdf-ext.org&quot;&gt;RDF-Ext&lt;/a&gt; (RDF JS libs) nearby, and in there Bergi has a [RdfNetwork component]&lt;a href=&quot;https://examples.rdf-ext.org/rdf-elements/&quot;&gt;RDF-Ext&lt;/a&gt;. This is built on &lt;a href=&quot;https://js.cytoscape.org/&quot;&gt;Cytoscape.js&lt;/a&gt;, which is a mature renderer from bioinformatics circles, so should be plenty good on scalability and scatterplotty things.&lt;/p&gt;
&lt;p&gt;#:todo what was that cytoscape plugin that looked good for pipelines from somewhere around Eclipse?
#:todo check out the cytoscape/&lt;a href=&quot;https://mermaid.js.org/ecosystem/integrations-community.html&quot;&gt;mermaid integrations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ok, I guess I should actually look at what I&amp;#39;ve got for #:semem code so far...&lt;/p&gt;

&lt;/article&gt;
&lt;p class=&quot;post-title h-cinzel&quot;&gt;
    &lt;a href=&quot;&quot;&gt;
        
    &lt;/a&gt;
&lt;/p&gt; &lt;em&gt;&lt;/em&gt;</content>
    </entry>
    
    <entry>
        <title>Entry 2024-12-29_prill</title>
        <link href="https://danny.ayers.name/entries/2024-12-29_prill.html"/>
        <id>https://danny.ayers.name/entries/2024-12-29_prill.html</id>
        <updated>2024-12-30T12:04:15.464Z</updated>
        <content type="html">&lt;!-- POST CONTENT TEMPLATE --&gt;

&lt;article class=&quot;post-content&quot;&gt;
        &lt;a href=&quot;&quot;&gt;#&lt;/a&gt;
    &lt;h1&gt;Prill&lt;/h1&gt;
&lt;p&gt;#:TIL the word &lt;a href=&quot;https://en.wiktionary.org/wiki/prill&quot;&gt;prill&lt;/a&gt;, &amp;quot;a pellet, a granule, a small bead&amp;quot;, while watching this: &lt;a href=&quot;https://youtu.be/dhW4XFGQB4o&quot;&gt;Iron knife made from bacteria&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;The ety of #:prill :&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Unknown. OED mentions Cornish pryl (“sheep-droppings”) as a likely loan from English.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;CF. #:nurdle&lt;/p&gt;
&lt;p&gt;The guy&amp;#39;s &lt;a href=&quot;https://primitivetechnology.wordpress.com/&quot;&gt;Primitive Technology blog&lt;/a&gt; and &lt;a href=&quot;https://youtube.com/@primitivetechnology9550&quot;&gt;videos&lt;/a&gt; &lt;strong&gt;turn on subtitles&lt;/strong&gt; are great for us &lt;em&gt;poltroniste&lt;/em&gt;  (armchair enthusiasts (including chair-oriented politicians)).&lt;/p&gt;
&lt;p&gt;Having to look it up, this came to mind:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Text is cheap, knowledge is worth it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/article&gt;
&lt;p class=&quot;post-title h-cinzel&quot;&gt;
    &lt;a href=&quot;&quot;&gt;
        
    &lt;/a&gt;
&lt;/p&gt; &lt;em&gt;&lt;/em&gt;</content>
    </entry>
    
    <entry>
        <title>Entry 2024-12-29_pivots</title>
        <link href="https://danny.ayers.name/entries/2024-12-29_pivots.html"/>
        <id>https://danny.ayers.name/entries/2024-12-29_pivots.html</id>
        <updated>2024-12-30T12:04:15.464Z</updated>
        <content type="html">&lt;!-- POST CONTENT TEMPLATE --&gt;

&lt;article class=&quot;post-content&quot;&gt;
        &lt;a href=&quot;&quot;&gt;#&lt;/a&gt;
    &lt;h1&gt;Pivots&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Thinking aloud to find the actionables&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Two artificial deadlines I want to set myself. I&amp;#39;ve got some vocab in-progress for expressing these, the term being #:pivot. There are two dates coming soon for good markers :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Poggi Pivot : Mari&amp;#39;s birthday, 2024-01-05&lt;/li&gt;
&lt;li&gt;Proof Pivot : my birthday, 2024-01-14&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first one, primary goal is getting &lt;strong&gt;some&lt;/strong&gt; docs online for &lt;strong&gt;all&lt;/strong&gt; my currently active projects. Skeleton summaries/placeholders at minimum, the #:what, a little #:how. But I really want to capture how they relate to each other, the #:why. &lt;/p&gt;
&lt;p&gt;The second one, primary goal is creating a proof-of-concept for each project. The minimum here again is little more than skeletal placeholders. But again, capture of the interrelations is the part I&amp;#39;m aiming for. Get the overall dependency graph formalised enough that it can be used in planning. It&amp;#39;s twisty.&lt;/p&gt;
&lt;p&gt;Ok, I&amp;#39;ve got actionables (not far off what I was doing already). For #:PoggiPivot, first step is to wire up all the #:postcraft spaces, make them live.
In quasi-parallel, towards #:ProofPivot, two must-haves : write down core project description for each, maybe in a proposal-like form; write down the #:poc requirements.&lt;/p&gt;
&lt;p&gt;This lot in itself is a major grind. But I need to remind myself where I&amp;#39;m up to with everything. Best way to do this and keep it interesting is to see what I have noted for next step #:todos, code up the next little step.&lt;/p&gt;
&lt;p&gt;I just did this with #:tbox. While going over it with Claude he gave me an architectural diagram (in Mermaid format) which I found really helpful. I&amp;#39;ve already got visualisations on the lists, but I reckon this is an aspect I should revisit very soon, to help with &lt;em&gt;everything else&lt;/em&gt;.&lt;/p&gt;

&lt;/article&gt;
&lt;p class=&quot;post-title h-cinzel&quot;&gt;
    &lt;a href=&quot;&quot;&gt;
        
    &lt;/a&gt;
&lt;/p&gt; &lt;em&gt;&lt;/em&gt;</content>
    </entry>
    
</feed>